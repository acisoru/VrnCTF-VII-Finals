from flask import Flask, request, jsonify, render_template
import os
import kubernetes.client
from kubernetes.client.rest import ApiException
import logging
import time

app = Flask(__name__, template_folder='templates')
logging.basicConfig(level=logging.INFO)

# Configure Kubernetes client
try:
    # Try to load in-cluster configuration
    kubernetes.config.load_incluster_config()
    logging.info("Loaded in-cluster Kubernetes configuration")
except:
    # Fallback to kubeconfig
    kubernetes.config.load_kube_config()
    logging.info("Loaded kubeconfig Kubernetes configuration")

api_instance = kubernetes.client.BatchV1Api()
core_v1 = kubernetes.client.CoreV1Api()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/check')
def check_host():
    host = request.args.get('host', '')
    if not host:
        return jsonify({"error": "No host provided"}), 400
    
    job_name = f"curl-{hash(host) & 0xffffffff}"
    
    # It's not like K8s uses yaml anyway
    job = {
        "apiVersion": "batch/v1",
        "kind": "Job",
        "metadata": {
            "name": job_name
        },
        "spec": {
            "ttlSecondsAfterFinished": 100,
            "template": {
                "spec": {
                    "containers": [
                        {
                            "name": "curl",
                            "image": "curlimages/curl:latest",
                            # ATTENTION PLEASE! As you can see, there is ZERO sanitization
                            "command": ["/bin/sh", "-c", f"curl -s -o /dev/null {host} && echo '{job_name}: Host {host} is reachable' || echo '{job_name}: Host {host} is not reachable'"],
                            "resources": {
                                "limits": {
                                    "memory": "64Mi",
                                    "cpu": "100m"
                                }
                            }
                        }
                    ],
                    "restartPolicy": "Never"
                }
            }
        }
    }
    
    try:
        # Create the job
        api_instance.create_namespaced_job(namespace="default", body=job)
        logging.info(f"Created job {job_name}")
        
        # Wait for job to complete
        status = "Running"
        result = "Pending"
    
        for _ in range(30):  # Every second wait for response
            time.sleep(1)
            job_status = api_instance.read_namespaced_job_status(name=job_name, namespace="default")
            
            if job_status.status.succeeded is not None and job_status.status.succeeded > 0:
                status = "Completed"
                
                # Get logs from the pod
                selector = f"job-name={job_name}"
                pods = core_v1.list_namespaced_pod(namespace="default", label_selector=selector)
                
                if pods.items:
                    pod_name = pods.items[0].metadata.name
                    result = core_v1.read_namespaced_pod_log(name=pod_name, namespace="default")  # this api is the worst
                break
                
            if job_status.status.failed is not None and job_status.status.failed > 0:
                status = "Failed"
                result = "Health check failed"
                break
        
        # Oh boi I sure do think job ttl will be enough to clean up orphan jobs, that are DEFINITELY not stuck at some broken shell or anything...
        
        return jsonify({
            "status": status,
            "result": result,
            "host": host
        })
        
    except ApiException as e:
        logging.error(f"Exception when calling Kubernetes API: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=os.environ.get('DEBUG', 'False').lower() == 'true')
